{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cfh9kVB-rpjo"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/main/images2/NorthwesternHeader.png?raw=1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRAx1f1ir1St"
   },
   "source": [
    "## MSDS458 Research Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQnyR1_aFk-t"
   },
   "source": [
    "### Analyze AG_NEWS_SUBSET Data <br>\n",
    "\n",
    "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity.<br>\n",
    "\n",
    "For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html<br>\n",
    "\n",
    "\n",
    "The AG's news topic classification dataset is constructed by choosing 4 largest classes (**World**, **Sports**, **Business**, and **Sci/Tech**) from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.<br>\n",
    "\n",
    "Homepage: https://arxiv.org/abs/1509.01626<br>\n",
    "\n",
    "Source code: tfds.text.AGNewsSubset\n",
    "\n",
    "Versions:\n",
    "\n",
    "1.0.0 (default): No release notes.\n",
    "Download size: 11.24 MiB\n",
    "\n",
    "Dataset size: 35.79 MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGBlk-2-NaCn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>More Technical</b>: Throughout the notebook. This types of boxes provide more technical details and extra references about what you are seeing. They contain helpful tips, but you can safely skip them the first time you run through the code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-Jjr3rNr5PR"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from packaging import version\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKucNfkrsqrP"
   },
   "source": [
    "## Verify TensorFlow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook requires TensorFlow 2.0 or above\")\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqAEjDjgzP0_"
   },
   "source": [
    "## Visualization Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation_report(test_labels, predictions):\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(test_labels, predictions))\n",
    "    print('Accuracy Score: {}'.format(accuracy_score(test_labels, predictions)))\n",
    "    print('Root Mean Square Error: {}'.format(np.sqrt(MSE(test_labels, predictions))))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.75,  cbar=False, ax=ax,cmap='Blues',linecolor='white')\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mFyXjJmtJ3E"
   },
   "source": [
    "## Mount Google Drive to Colab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e2r4vSYzP1B"
   },
   "source": [
    "### Load AG News Subset Data\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b> ag_news_subset</b><br>\n",
    "    See https://www.tensorflow.org/datasets/catalog/ag_news_subset\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cL3ewSkAF4yF"
   },
   "source": [
    "Get all the words in the documents (as well as the number of words in each document) by using the encoder to get the indices associated with each token and then translating the indices to tokens. But first we need to get the \"unpadded\" new articles so that we can get their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/datasets/splits\n",
    "# The full `train` and `test` splits, interleaved together.\n",
    "ri = tfds.core.ReadInstruction('train') + tfds.core.ReadInstruction('test')\n",
    "dataset_all, info = tfds.load('ag_news_subset', with_info=True,  split=ri, as_supervised=True)\n",
    "text_only_dataset_all=dataset_all.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bzs2FWbQIf5m"
   },
   "source": [
    "###  EDA AG News Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7514-j4I4B-"
   },
   "source": [
    "**Get information about the ag_news_subset dataset. We combined the training and test data for a total of 127,600 news articles.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.as_dataframe(dataset_all.take(10),info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoG_cOcMMKgC"
   },
   "source": [
    "### Review Labels (Categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories =dict(enumerate(info.features[\"label\"].names))\n",
    "print(f'Dictionary: ',categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0TskodXzP1F"
   },
   "source": [
    "### Review Class Balance\n",
    "The 127,600 news articles are evenly distributed among the 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categories = [categories[label] for label in dataset_all.map(lambda text, label: label).as_numpy_iterator()]\n",
    "Counter(train_categories).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ez6OmV7zP1G"
   },
   "source": [
    "### Preprocess Dataset for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stopwords(input_text):\n",
    "    lowercase = tf.strings.lower(input_text)\n",
    "    stripped_punct = tf.strings.regex_replace(lowercase\n",
    "                                  ,'[%s]' % re.escape(string.punctuation)\n",
    "                                  ,'')\n",
    "    return tf.strings.regex_replace(stripped_punct, r'\\b(' + r'|'.join(STOPWORDS) + r')\\b\\s*',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords',quiet=True)\n",
    "STOPWORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2Dapu4RzP1H"
   },
   "source": [
    "### TextVectorization and Adapt() Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_tokens = None\n",
    "text_vectorization=layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_stopwords\n",
    ")\n",
    "text_vectorization.adapt(text_only_dataset_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "doc_sizes = []\n",
    "corpus = []\n",
    "for example, _ in dataset_all.as_numpy_iterator():\n",
    "  enc_example = text_vectorization(example)\n",
    "  doc_sizes.append(len(enc_example))\n",
    "  corpus+=list(enc_example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(corpus)} words in the corpus of {len(doc_sizes)} news articles.\")\n",
    "print(f\"Each news article has between {min(doc_sizes)} and {max(doc_sizes)} tokens in it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(text_vectorization.get_vocabulary())} vocabulary words in the corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(text_vectorization.get_vocabulary())\n",
    "print(vocab[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "plt.hist(doc_sizes, bins=20,range = (0,80))\n",
    "plt.xlabel(\"Tokens Per Document\")\n",
    "plt.ylabel(\"Number of AG News Articles\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Stamp\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Print the formatted time\n",
    "print(\"Last Run:\", formatted_time)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
