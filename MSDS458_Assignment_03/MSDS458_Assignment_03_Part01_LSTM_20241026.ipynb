{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHJ-2a8LRro5"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/main/images2/NorthwesternHeader.png?raw=1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6cApW345ukg"
   },
   "source": [
    "## MSDS458 Research Assignment 3 - Part 01 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hblbilyx5ukg"
   },
   "source": [
    "### Analyze AG_NEWS_SUBSET Data <br>\n",
    "\n",
    "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity.<br>\n",
    "\n",
    "For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html<br>\n",
    "\n",
    "\n",
    "The AG's news topic classification dataset is constructed by choosing 4 largest classes (**World**, **Sports**, **Business**, and **Sci/Tech**) from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.<br>\n",
    "\n",
    "Homepage: https://arxiv.org/abs/1509.01626<br>\n",
    "\n",
    "Source code: tfds.text.AGNewsSubset\n",
    "\n",
    "Versions:\n",
    "\n",
    "1.0.0 (default): No release notes.\n",
    "Download size: 11.24 MiB\n",
    "\n",
    "Dataset size: 35.79 MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95DGbFzR5uki"
   },
   "source": [
    "### References\n",
    "1. Deep Learning with Python, Francois Chollet (https://learning.oreilly.com/library/view/deep-learning-with/9781617296864/)\n",
    " * Chapter 10: Deep learning for time series\n",
    " * Chapter 11: Deep learning for text\n",
    "2. Deep Learning A Visual Approach, Andrew Glassner (https://learning.oreilly.com/library/view/deep-learning/9781098129019/)\n",
    " * Chapter 19: Recurrent Neural Networks\n",
    " * Chapter 20: Attention and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT8kwmgNRro7"
   },
   "source": [
    "### Processing words as a sequence: The sequence model approach\n",
    "\n",
    "To implement a sequence model, you’d start by representing your input samples as sequences of integer indices (one integer standing for one word). Then, you’d map each integer to a vector to obtain vector sequences. Finally, you’d feed these sequences of vectors into a stack of layers that could cross-correlate features from adjacent vectors, such as a 1D convnet, a RNN, or a Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4AQ7tuV5ukj"
   },
   "source": [
    "For some time around 2016–2017, bidirectional RNNs (in particular, `bidirectional LSTMs`) were considered to be the state of the art for sequence modeling. However, nowadays sequence modeling is almost universally done with `Transformers`.\n",
    "\n",
    "F. Chollet: \"One-dimensional convnets were never very popular in NLP, even though, a residual stack of depthwise-separable 1D convolutions can often achieve comparable performance to a bidirectional LSTM, at a greatly reduced computational cost.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ydgzc1l15ukl"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as k\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNt8VbLK5ukp"
   },
   "source": [
    "\\### Verify TensorFlow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook requires TensorFlow 2.0 or above\")\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPwJHwRh4TJU"
   },
   "source": [
    "### Stopword Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stopwords(input_text):\n",
    "    lowercase = tf.strings.lower(input_text)\n",
    "    stripped_punct = tf.strings.regex_replace(lowercase\n",
    "                                  ,'[%s]' % re.escape(string.punctuation)\n",
    "                                  ,'')\n",
    "    return tf.strings.regex_replace(stripped_punct, r'\\b(' + r'|'.join(STOPWORDS) + r')\\b\\s*',\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ty6hkES4TJU"
   },
   "source": [
    "### Plotting Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_training_curves(training, validation, title, subplot):\n",
    "  ax = plt.subplot(subplot)\n",
    "  ax.plot(training)\n",
    "  ax.plot(validation)\n",
    "  ax.set_title('model '+ title)\n",
    "  ax.set_ylabel(title)\n",
    "  ax.set_xlabel('epoch')\n",
    "  ax.legend(['training', 'validation'])\n",
    "\n",
    "def print_validation_report(test_labels, predictions):\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(test_labels, predictions))\n",
    "    print('Accuracy Score: {}'.format(accuracy_score(test_labels, predictions)))\n",
    "    print('Root Mean Square Error: {}'.format(np.sqrt(MSE(test_labels, predictions))))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(16,12))\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.75,  cbar=False, ax=ax,cmap='Blues',linecolor='white')\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1Gn6wYG5uks"
   },
   "source": [
    "### Mount Google Drive to Colab environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1eOMhjR5pO9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvqFLia1Rro9"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,info=\\\n",
    "tfds.load('ag_news_subset', with_info=True,  split=['train[:95%]','train[95%:]', 'test'],batch_size = 32\n",
    "          , as_supervised=True)\n",
    "\n",
    "train_ds, val_ds, test_ds = dataset\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mq7Q6zlFRro_"
   },
   "source": [
    "### Preparing Integer Sequence Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords',quiet=True)\n",
    "STOPWORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 96\n",
    "max_tokens = 1000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    "    standardize=custom_stopwords\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBL0CkaB5ukv"
   },
   "source": [
    "### Bi-directional RNN\n",
    "\n",
    "When translating in real-time, it would help to have access to worlds towards the end of a sentence, say, as well as earlier words in the sentence. One way to use the later words in the sentence is to feed the words into our RNN backward. So if we create two independent RNNs, we can feed one the words in their forward, or natural order, and the second gets their words in the revser order. This is the idea behind `Bi-directional RNNS`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuP7exeZ5ukw"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/main/images2/BidirectionalRNN.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTprEi3bRrpD"
   },
   "source": [
    "### Understanding word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmHdhUnM5ukz"
   },
   "source": [
    "When you encode something via `one-hot encoding`, you’re making a feature-engineering decision. You’re injecting into your model a fundamental assumption about the structure of your feature space. That assumption is that the different tokens you’re encoding are all independent from each other: indeed, one-hot vectors are all orthogonal to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "webaWkZJ5ukz"
   },
   "source": [
    "However, in a reasonable word vector space, you would expect synonyms to be embedded into similar word vectors, and in general, you would expect the geometric distance  between any two word vectors to relate to the “semantic distance” between the associated words.\n",
    "\n",
    "Words that mean different things should lie far away from each other, whereas related words should be closer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-W1sFgg5ukz"
   },
   "source": [
    "`Word embeddings` are vector representations of words that achieve exactly this: they map human language into a structured geometric space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0egzHasW5ukz"
   },
   "source": [
    "Whereas the vectors obtained through `one-hot encoding` are *binary*, *sparse*, and *very high-dimensional* (the same dimensionality as the number of words in the vocabulary), `word embeddings` are *low-dimensional floating-point vectors* (that is, `dense vectors`, as opposed to `sparse vectors`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHU4tS3b5uk0"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/main/images2/EmbeddingsSparse.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "im1j5PRE5uk0"
   },
   "source": [
    "### Two ways to obtain word embeddings\n",
    "\n",
    "1. `Learn word embeddings jointly with the main task you care about` (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the weights of a neural network.\n",
    "2. Load into your model word embeddings that were precomputed using a different machine learning task than the one you’re trying to solve. These are called `pretrained word embeddings`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j69C96c65uk1"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>tf.keras.layers.Embedding</b><br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0gOTdNnRrpF"
   },
   "source": [
    "### Understanding Padding and Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOudeXoa5uk4"
   },
   "source": [
    "One thing that’s slightly hurting model performance here is that our input sequences are full of zeros. This comes from our use of the `output_sequence_length=max_ length` option in TextVectorization (with `max_length equal` to 150): sentences longer than 150 tokens are truncated to a length of 150 tokens, and sentences shorter than 150 tokens are padded with zeros at the end so that they can be concatenated together with other sequences to form contiguous batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM95tdc65uk4"
   },
   "source": [
    "The RNN that looks at the tokens in their natural order will spend its last iterations seeing only vectors that encode padding—possibly for several hundreds of iterations if the original sentence was short. The information stored in the internal state of the RNN will gradually fade out as it gets exposed to these meaningless inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U93_Szgn5uk4"
   },
   "source": [
    "We need some way to tell the RNN that it should skip these iterations. There’s an API for that: `masking`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1SxQSW85uk4"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>tf.keras.layers.Masking</b><br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGe0pvoxRrpF"
   },
   "source": [
    "### Model Leveraging Embedding Layer With Masking Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.clear_session()\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens\n",
    "                            ,output_dim=256\n",
    "                            ,mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"SparseCategoricalCrossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"LSTM.keras\",save_best_only=True)\n",
    "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "]\n",
    "history=model.fit(int_train_ds, validation_data=int_val_ds, epochs=200, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"LSTM.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixoqrCPQ4TJh"
   },
   "source": [
    "### Post Modeling Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history_df['accuracy'], history_df['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history_df['loss'], history_df['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Stamp\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Print the formatted time\n",
    "print(\"Last Run:\", formatted_time)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
